{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing dataset...\n",
      "Loading dataset...\n",
      "Preparing datasets...\n",
      "Class Distribution in Train Dataset: {np.int64(0): np.int64(834), np.int64(1): np.int64(806)}\n",
      "Final Train set size: 1640, Test set size: 410\n",
      "Initializing Classical SVM...\n",
      "Applying PCA...\n",
      "PCA Variance Ratio: 1.0000\n",
      "Training Classical SVM...\n",
      "Starting hyperparameter tuning...\n",
      "Fitting 4 folds for each of 240 candidates, totalling 960 fits\n",
      "Best Parameters: {'C': 100, 'degree': 2, 'gamma': 1, 'kernel': 'rbf'}\n",
      "Best Cross-Validation Score: 0.9409\n",
      "SVM training complete. Time taken: 70.90 minutes.\n",
      "Memory Usage: 2.98 MB (during training)\n",
      "Evaluating Classical SVM...\n",
      "SVM evaluation complete. Time taken: 0.00 minutes.\n",
      "Memory Usage: 0.02 MB (during evaluation)\n",
      "SVM Accuracy: 0.9463\n",
      "Precision: 0.9479\n",
      "Recall: 0.9463\n",
      "F1 Score: 0.9463\n",
      "Confusion Matrix:\n",
      " [[192  17]\n",
      " [  5 196]]\n",
      "Saving SVM model...\n",
      "SVM model saved to csvm_midmodel.pkl\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import psutil  # For memory tracking\n",
    "import pandas as pd\n",
    "import joblib  # For model saving/loading\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "import time\n",
    "import os\n",
    "\n",
    "class ClassicalSVM:\n",
    "    def __init__(self, data, pca_components):\n",
    "        print(\"Initializing Classical SVM...\")\n",
    "        self.data = data\n",
    "        self.pca_components = pca_components\n",
    "        self.apply_pca()\n",
    "\n",
    "    def apply_pca(self):\n",
    "        \"\"\"Reduce feature dimensions using PCA\"\"\"\n",
    "        print(\"Applying PCA...\")\n",
    "        pca = PCA(n_components=self.pca_components)\n",
    "        self.data.train_features = pca.fit_transform(self.data.train_features)\n",
    "        self.data.test_features = pca.transform(self.data.test_features)\n",
    "        print(f\"PCA Variance Ratio: {sum(pca.explained_variance_ratio_):.4f}\")\n",
    "\n",
    "    \n",
    "    @staticmethod\n",
    "    def svm_hyperparameter_tuning(train_features, train_labels):\n",
    "        print(\"Starting hyperparameter tuning...\")\n",
    "\n",
    "        param_grid = {\n",
    "        'C': [0.1, 1, 10, 100],\n",
    "        'kernel': ['linear', 'rbf', 'poly', 'sigmoid'],\n",
    "        'gamma': ['scale', 'auto', 0.01, 0.1, 1],\n",
    "        'degree': [2, 3, 4]  # Only relevant for 'poly'\n",
    "        }\n",
    "\n",
    "        svc = SVC()\n",
    "        grid_search = GridSearchCV(svc, param_grid, cv=4, scoring='accuracy', verbose=1, n_jobs=-1)\n",
    "        grid_search.fit(train_features, train_labels)\n",
    "\n",
    "        print(f\"Best Parameters: {grid_search.best_params_}\")\n",
    "        print(f\"Best Cross-Validation Score: {grid_search.best_score_:.4f}\")\n",
    "\n",
    "        return grid_search.best_estimator_\n",
    "\n",
    "    def fit(self):\n",
    "        \"\"\"Train classical SVM with memory tracking\"\"\"\n",
    "        print(\"Training Classical SVM...\")\n",
    "\n",
    "        process = psutil.Process()\n",
    "        mem_before = process.memory_info().rss / (1024 * 1024)  # in MB\n",
    "\n",
    "        start_time = time.time()\n",
    "        self.svc = SVC(kernel='rbf', C=1.0)\n",
    "        self.svc = self.svm_hyperparameter_tuning(self.data.train_features, self.data.train_labels)\n",
    "        end_time = time.time()\n",
    "\n",
    "        mem_after = process.memory_info().rss / (1024 * 1024)\n",
    "\n",
    "        print(f\"SVM training complete. Time taken: {(end_time - start_time) / 60:.2f} minutes.\")\n",
    "        print(f\"Memory Usage: {mem_after - mem_before:.2f} MB (during training)\")\n",
    "\n",
    "    def evaluate(self):\n",
    "        \"\"\"Evaluate SVM using accuracy metrics and memory tracking\"\"\"\n",
    "        print(\"Evaluating Classical SVM...\")\n",
    "\n",
    "        process = psutil.Process()\n",
    "        mem_before = process.memory_info().rss / (1024 * 1024)\n",
    "\n",
    "        start_time = time.time()\n",
    "        predictions = self.svc.predict(self.data.test_features)\n",
    "        end_time = time.time()\n",
    "\n",
    "        mem_after = process.memory_info().rss / (1024 * 1024)\n",
    "\n",
    "        print(f\"SVM evaluation complete. Time taken: {(end_time - start_time) / 60:.2f} minutes.\")\n",
    "        print(f\"Memory Usage: {mem_after - mem_before:.2f} MB (during evaluation)\")\n",
    "\n",
    "        acc = accuracy_score(self.data.test_labels, predictions)\n",
    "        prec = precision_score(self.data.test_labels, predictions, average=\"weighted\")\n",
    "        rec = recall_score(self.data.test_labels, predictions, average=\"weighted\")\n",
    "        f1 = f1_score(self.data.test_labels, predictions, average=\"weighted\")\n",
    "        conf_matrix = confusion_matrix(self.data.test_labels, predictions)\n",
    "\n",
    "        print(f\"SVM Accuracy: {acc:.4f}\")\n",
    "        print(f\"Precision: {prec:.4f}\")\n",
    "        print(f\"Recall: {rec:.4f}\")\n",
    "        print(f\"F1 Score: {f1:.4f}\")\n",
    "        print(\"Confusion Matrix:\\n\", conf_matrix)\n",
    "\n",
    "        return {\n",
    "            \"accuracy\": acc,\n",
    "            \"precision\": prec,\n",
    "            \"recall\": rec,\n",
    "            \"f1_score\": f1,\n",
    "            \"confusion_matrix\": conf_matrix\n",
    "        }\n",
    "\n",
    "    def save_model(self, filename=\"csvm_midmodel.pkl\"):\n",
    "        \"\"\"Save the trained classical SVM model\"\"\"\n",
    "        if not hasattr(self, \"svc\") or self.svc is None:\n",
    "            print(\"Error: No trained model found. Train the SVM first.\")\n",
    "            return\n",
    "\n",
    "        print(\"Saving SVM model...\")\n",
    "        joblib.dump(self.svc, filename)\n",
    "        print(f\"SVM model saved to {filename}\")\n",
    "\n",
    "    def load_model(self, filename=\"csvm_midmodel.pkl\"):\n",
    "        \"\"\"Load a previously saved SVM model\"\"\"\n",
    "        if not os.path.exists(filename):\n",
    "            print(f\"Error: Model file {filename} not found.\")\n",
    "            return\n",
    "\n",
    "        print(\"Loading SVM model...\")\n",
    "        self.svc = joblib.load(filename)\n",
    "        print(f\"SVM model loaded from {filename}\")\n",
    "\n",
    "class Data:\n",
    "    def __init__(self, filename):\n",
    "        print(\"Initializing dataset...\")\n",
    "        self.filename = filename\n",
    "        self.load_data()\n",
    "        self.prepare_datasets()\n",
    "\n",
    "    def load_data(self):\n",
    "        \"\"\"Load dataset and apply necessary preprocessing\"\"\"\n",
    "        print(\"Loading dataset...\")\n",
    "        self.df = pd.read_csv(self.filename)\n",
    "\n",
    "    def prepare_datasets(self):\n",
    "        \"\"\"Split dataset into train/test and scale features\"\"\"\n",
    "        print(\"Preparing datasets...\")\n",
    "\n",
    "        features = self.df.drop(columns=[\"HAS ADHD\"]).to_numpy()\n",
    "        labels = self.df[\"HAS ADHD\"].to_numpy()\n",
    "\n",
    "        scaler = MinMaxScaler(feature_range=(0, 2 * np.pi))\n",
    "        features = scaler.fit_transform(features)\n",
    "\n",
    "        train_features, test_features, train_labels, test_labels = train_test_split(\n",
    "            features, labels, test_size=0.2, stratify=labels, random_state=42\n",
    "        )\n",
    "\n",
    "        unique, counts = np.unique(train_labels, return_counts=True)\n",
    "        class_distribution = dict(zip(unique, counts))\n",
    "        print(f\"Class Distribution in Train Dataset: {class_distribution}\")\n",
    "\n",
    "        self.train_features, self.train_labels = train_features, train_labels\n",
    "        self.test_features, self.test_labels = test_features, test_labels\n",
    "\n",
    "        print(f\"Final Train set size: {len(self.train_features)}, Test set size: {len(self.test_features)}\")\n",
    "\n",
    "\n",
    "# Load dataset and train Classical SVM\n",
    "dataset = Data('MLsheet - SRSno-avg.csv')\n",
    "svm_model = ClassicalSVM(dataset, pca_components=6)\n",
    "svm_model.fit()\n",
    "\n",
    "# Evaluate SVM performance\n",
    "metrics = svm_model.evaluate()\n",
    "\n",
    "# Save the trained model for later use\n",
    "svm_model.save_model()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
